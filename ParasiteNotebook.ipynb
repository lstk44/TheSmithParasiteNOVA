{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>The Smith Parasite - An Unknown Parasitic Disease</center>\n",
    "\n",
    "A new disease has recently been discovered by Dr. Smith, in England. You have been brought in to investigate.\n",
    "The disease has already affected more than 5000 people, with no apparent connection between them.\n",
    "\n",
    "The most common symptoms include fever and tiredness, but some infected people are asymptomatic. Regardless, this virus is being associated with post-disease conditions such as loss of speech, confusion, chest pain and shortness of breath.\n",
    "\n",
    "The conditions of the transmission of the disease are still unknown and there are no certainties of what leads a patient to suffer or not from it. Nonetheless, some groups of people seem more prone to be infected by the parasite than others.\n",
    "\n",
    "In this challenge, your goal is to build a predictive model that answers the question, “Who are the people more likely to suffer from the Smith Parasite?”. With that goal, you can access a small quantity of sociodemographic, health, and behavioral information obtained from the patients.\n",
    "\n",
    "As data scientists, your team is asked to analyze and transform the data available as needed and apply different models to answer the defined question in a more accurate way. Can you build a model that can predict if a patient will suffer, or not, from the Smith Disease?\n",
    "\n",
    "---\n",
    "\n",
    "### Datasets\n",
    "The training set should be used to build your machine learning models. In this set, you also have the ground truth associated to each patient, i.e., if the patient has the disease (Disease = 1) or not (Disease = 0).\n",
    "\n",
    "The test set should be used to see how well your model performs on unseen data. In this set you don’t have access to the ground truth, and the goal of your team is to predict that value (0 or 1) by using the model you created using the training set.\n",
    "\n",
    "The score of your predictions is the percentage of instances you correctly predict, using the **f1 score**.\n",
    "\n",
    "---\n",
    "\n",
    "### Deliverables\n",
    "\n",
    "You should submit a csv file with the number of instances in the test set, containing the columns [PatientID, Disease], and only those columns. <br>\n",
    "The Disease column should contain the prediction (“0” or “1”). Your last solution (the Jupiter notebook) should be submitted on Moodle.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import functools\n",
    "import math\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.model_selection import train_test_split, learning_curve, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import RocCurveDisplay, roc_auc_score, f1_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {dataset_name.replace('.xlsx', ''): pd.read_excel(f'Data/{dataset_name}') for dataset_name in os.listdir('Data') if 'xlsx' in dataset_name}        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(data.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frames = [data['train_demo'], data['train_habits'], data['train_health']]\n",
    "df = functools.reduce(lambda  left,right: pd.merge(left,right,on=['PatientID'], how='outer'), data_frames).set_index('PatientID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frames = [data['test_demo'], data['test_habits'], data['test_health']]\n",
    "df_test = functools.reduce(lambda  left,right: pd.merge(left,right,on=['PatientID'], how='outer'), data_frames).set_index('PatientID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df.isnull().sum(),df.eq('').sum()],keys=['Nulls','Empty Strings'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include=\"all\").T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "\n",
    "sns.pairplot(df, hue = 'Disease', markers = ['o', 's'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numeric_features = list(df.select_dtypes(include = np.number).columns)\n",
    "df_numeric_features.remove('Disease')\n",
    "\n",
    "fig, ax = plt.subplots(math.ceil(len(df_numeric_features)/4),4, figsize = (15,10))\n",
    "for ax, feat in zip(ax.flatten(), df_numeric_features):\n",
    "    ax.boxplot(df[feat], notch = True, patch_artist = True)\n",
    "    ax.set_title(feat)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Birth_Year'] < 1961]['Birth_Year'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Takeaways from EDA\n",
    "- Regarding the numeric features \"Physical_Health\" does have the highest correlation\n",
    "- There're outliers in the \"Birth_Year\" column. Probably not possible?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Baseline\n",
    "- only numeric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = df.drop(columns = ['Disease']), df['Disease']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xnumeric = x.select_dtypes(include = np.number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xval, ytrain, yval = train_test_split(xnumeric, y, random_state = 111 ,test_size = 0.3, shuffle = True , stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(max_iter = 500)\n",
    "model.fit(xtrain,ytrain)\n",
    "\n",
    "ypred = model.predict(xval)\n",
    "\n",
    "print(classification_report(yval, ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Preprocessing\n",
    "- Impute\n",
    "- One-Hot-Encoding\n",
    "- Scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding the Birth_Year column and it's strange outliers (e.g. 1869) we have several options.<br>\n",
    "Considering we're using the **quantile +/- 1.5 * interquantile range** or IQR method rule we only end up with 12 critical observations\n",
    "\n",
    "1. The easy way would be to just filter out the observation. In that case every observation below 1941 would get dropped\n",
    "2. Another way would be to just fill those few observation with the mode\n",
    "3. The last option and probably the best would be to assume 1869 to be a typo. We could change all Birth_Year observations from the 19th century to the 20th century - 1869 --> 1969"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(dataframe):\n",
    "\n",
    "    # fix Birth_Year errors (1869 > 1969)\n",
    "    dataframe['Birth_Year'] = [i + 100 if i < 1900 else i for i in dataframe['Birth_Year']]\n",
    "\n",
    "    # add Age column\n",
    "    dataframe['Age'] = [2022 - i for i in dataframe['Birth_Year']]\n",
    "\n",
    "    # add Gender column (1: Male, 0: Female)\n",
    "    # afterwards drop column Name\n",
    "    dataframe['Gender']  = [1 if i.split(' ')[0] == 'Mr.' else 0 for i in dataframe['Name']]\n",
    "    dataframe.drop(columns = ['Name'], inplace = True)\n",
    "\n",
    "    # add column population density\n",
    "    dataframe['Region'] = [i.lower() for i in dataframe['Region']]\n",
    "    dataframe['Region_Density'] = [dataframe['Region'].value_counts(normalize = True)[i] for i in dataframe['Region']]\n",
    "\n",
    "    # encode Smoking_Habit & Exercise to binary (1: Yes, 0: No)\n",
    "    dataframe['Smoking_Habit'] = [1 if i == 'Yes' else 0 for i in dataframe['Smoking_Habit']]\n",
    "    dataframe['Exercise'] = [1 if i == 'Yes' else 0 for i in dataframe['Exercise']]\n",
    "\n",
    "    # try knn imputing?\n",
    "    # impute missing values in column \"Education\" with mode\n",
    "    dataframe['Education'].fillna(dataframe['Education'].mode()[0], inplace=True)\n",
    "\n",
    "    # split x in numeric and categorical features\n",
    "    xnumeric, xcategorical = dataframe.select_dtypes(include = np.number), dataframe.select_dtypes(exclude = np.number)\n",
    "\n",
    "    # One-hot-encoding categorical features\n",
    "    encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "    x_cat_encoded = pd.DataFrame(encoder.fit_transform(xcategorical).toarray(), columns = encoder.get_feature_names_out(), index = xcategorical.index) \n",
    "\n",
    "    # Scale numeric features\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    x_num_scaled = pd.DataFrame(scaler.fit_transform(xnumeric), index = xnumeric.index ,columns = xnumeric.columns)\n",
    "\n",
    "    xpreprocessed = pd.concat([x_num_scaled, x_cat_encoded], axis = 1)\n",
    "\n",
    "    return xpreprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = df.drop(columns = ['Disease']), df['Disease']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xpreprocessed = preprocessing(x)\n",
    "\n",
    "y = y.loc[xpreprocessed.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Train, test and validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xval, ytrain, yval = train_test_split(xpreprocessed, y, random_state = 222 , test_size = 0.3, shuffle = True , stratify = y)\n",
    "xval, xtest, yval, ytest = train_test_split(xval, yval, random_state = 333, test_size = 0.5, shuffle = True, stratify = yval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(max_iter = 500)\n",
    "model.fit(xtrain,ytrain)\n",
    "\n",
    "ypred = model.predict(xval)\n",
    "\n",
    "print(classification_report(yval, ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "\n",
    "##### Correlation Matrix - here we're able to just LabelEncode our categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "xnumeric, xcategorical = x.select_dtypes(include = np.number), x.select_dtypes(exclude = np.number)\n",
    "encoder = OrdinalEncoder()\n",
    "x_cat_encoded = pd.DataFrame(encoder.fit_transform(xcategorical), index = xcategorical.index, columns = xcategorical.columns)\n",
    "\n",
    "xcorrelation = pd.concat([xnumeric, x_cat_encoded], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = pd.concat([xcorrelation, y], axis = 1).corr(method = 'spearman')\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(correlation, dtype=bool))\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "fig, ax = plt.subplots(figsize=(20, 15))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(correlation, annot = True, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(random_state = 444)\n",
    "rfe = RFECV(model, cv = StratifiedKFold(10), scoring = 'f1')\n",
    "rfe.fit(xtrain, ytrain)\n",
    "opt_features = list(rfe.get_feature_names_out(input_features = list(xtrain.columns)))\n",
    "print(f'Best features: {[i for i in opt_features]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_rfe = rfe.transform(xtrain)\n",
    "xval_rfe = rfe.transform(xval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_auc(classifiers, xtrain, ytrain, xval, yval):\n",
    "    '''\n",
    "    Plots ROC/AUC\n",
    "    classifiers input --> {'Logistic Regression': LogisticRegression(),...}\n",
    "    '''\n",
    "\n",
    "    fig, ax = plt.subplots(1, figsize=(15, 10))\n",
    "    for name, clf in classifiers.items():\n",
    "        clf.fit(xtrain, ytrain)\n",
    "        RocCurveDisplay.from_estimator(clf, xval, yval, ax=ax, name=name)\n",
    "    ax.set_title('Receiver Operating Characteristic (ROC)')\n",
    "    ax.plot([0,1], [0,1], linestyle='--')\n",
    "    plt.show()\n",
    "\n",
    "classifiers = {\n",
    "                'Logistic Regression': LogisticRegression(),\n",
    "                'SVM': SVC(),\n",
    "                'Neural Network': MLPClassifier(max_iter = 500),\n",
    "                'Decision Tree': DecisionTreeClassifier(),\n",
    "                'Random Forest': RandomForestClassifier(),\n",
    "                'GradientBoostingClassifier': GradientBoostingClassifier()\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc(classifiers, xtrain_rfe, ytrain, xval_rfe, yval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_curves(estimator, X, Y):\n",
    "    '''\n",
    "    Plots learning curve\n",
    "    '''\n",
    "\n",
    "    train_sizes, train_scores, validation_scores = learning_curve(estimator, X, Y, cv = 10, scoring = 'f1', train_sizes = np.arange(.05,1,.05))\n",
    "    train_mean, test_mean, train_std, test_std = np.mean(train_scores, axis=1), np.mean(validation_scores, axis=1), np.std(train_scores, axis=1), np.std(validation_scores, axis=1)\n",
    "\n",
    "    plt.subplots(1, figsize=(10,10))\n",
    "    plt.plot(train_sizes, train_mean, color='salmon',  label='Training score', marker = 'o')\n",
    "    plt.plot(train_sizes, test_mean, color='olive', label='Cross-validation score', marker = 's')\n",
    "    plt.title('Learning Curve')\n",
    "    plt.xlabel('Training Set Size')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "learning_curves(DecisionTreeClassifier(random_state = 555), xpreprocessed, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Hyperparamter tuning\n",
    "\n",
    "- GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "                'bootstrap': [True, False],\n",
    "                'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
    "                'max_features': ['auto', 'sqrt'],\n",
    "                'min_samples_leaf': [1, 2, 4],\n",
    "                'min_samples_split': [2, 5, 10],\n",
    "                'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]\n",
    "            }\n",
    "\n",
    "\n",
    "model = RandomForestClassifier(random_state = 666)\n",
    "rand_search = RandomizedSearchCV(estimator = model, param_distributions = param_grid, scoring = 'f1', n_iter = 100, cv = 3, verbose=2, random_state=777, n_jobs = -1)\n",
    "rand_search.fit(xtrain_rfe, ytrain)\n",
    "\n",
    "print(str(rand_search.best_params_).replace('{','').replace('}','').replace(\"'\",\"\").replace(':','='))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Train model with best classifier, optimized feature selection and hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(**rand_search.best_params_)\n",
    "model.fit(xtrain_rfe, ytrain)\n",
    "\n",
    "validation_pred = model.predict(xval_rfe)\n",
    "\n",
    "print(classification_report(yval, validation_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Final train with all of our available data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xfull = xpreprocessed\n",
    "\n",
    "xfull_rfe = rfe.transform(xfull)\n",
    "\n",
    "model = RandomForestClassifier(**rand_search.best_params_)\n",
    "model.fit(xfull_rfe, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Predict test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtest = df_test\n",
    "\n",
    "xtest_preprocessed = preprocessing(xtest)\n",
    "\n",
    "xtest_rfe = rfe.transform(xtest_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = model.predict(xtest_rfe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Create CSV-file for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_submission = pd.concat([pd.Series(xtest.index),pd.Series(ypred)], axis = 1)\n",
    "# df_submission.rename(columns = {0:'Disease'}, inplace = True)\n",
    "# df_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_submission.to_csv('Group01_Version03.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8540791a84c9dd273039c82e4c4906b2f22bd80b7ff15c37ffcc5926b6e4ab9b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
